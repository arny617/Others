{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Apply Topic Modelling (using LDA, NMF, TFIDF) on scraped SEC reports, to find topics discussed among all the reports </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "# Plotting tools\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim  \n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow\n",
    "import pickle\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from tqdm import notebook\n",
    "import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"sec_10k_new2\")\n",
    "#.chdir(\"sec_10k_new\")\n",
    "files_req = [x for x in os.listdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carnival'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_req[0].split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(html_text):\n",
    "    soup = BeautifulSoup(html_text)\n",
    "    for script in soup([\"script\"]): \n",
    "        script.extract()\n",
    "    fin_text = soup.get_text()\n",
    "    fin_text = fin_text.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"\\xa0\",\"\").replace(\"\\ufeff\",\"\")\n",
    "    return(fin_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad63cb5bd6c4cdfafb58a01d410e9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df = pd.DataFrame()\n",
    "i = 0\n",
    "for file in notebook.tqdm(files_req):\n",
    "    html_file = open(file, 'r', encoding='utf-8',errors='ignore')\n",
    "    html_body = html_file.read() \n",
    "    clean_text = clean_html(html_body)\n",
    "    doc_df = doc_df.append(pd.Series([\"doc_\"+str(i),clean_text]),ignore_index=True)\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc_df.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df.columns = [\"doc_name\",\"text\"]\n",
    "joblib.dump(doc_df,\"doc_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_blob):\n",
    "    \"\"\"\n",
    "    This function aims to perform a series of text processing tasks - \n",
    "    For each paragraph\n",
    "        a)basic pre-processing and removal of punctuations\n",
    "        b)perform POS tagiing\n",
    "        c)remove stopwords, non alphabetical character sequences and proper nouns\n",
    "    \n",
    "    Input : Simple raw text corpus\n",
    "    Output : list of processed text\n",
    "    \"\"\"\n",
    "    stopwords = sw.words(\"english\")\n",
    "    ps = SnowballStemmer(\"english\")\n",
    "    #ps = PorterStemmer()\n",
    "    para = ' '.join(simple_preprocess(text_blob,deacc = True)) #Basic preprocessing and removal of punctuations\n",
    "    tagged_list = nltk.tag.pos_tag(para.split())\n",
    "    fin_text_blob = []\n",
    "    for word,tag in tagged_list:\n",
    "        if((word.lower() not in stopwords) and (word.isalpha()) and (len(word) >= 3) and (tag != \"NNP\" and tag != \"NNPS\")):\n",
    "            fin_text_blob.extend([ps.stem(word.lower())])\n",
    "    return(fin_text_blob) #This function hence returns the original text blob, processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100b6ac2cfcf42e59d21a211f6506de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_list = []\n",
    "for i in notebook.tqdm(range(doc_df.shape[0])):\n",
    "    text_blob = clean_text(doc_df.text[i])\n",
    "    text_list.extend([text_blob])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bigram module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "bigram = gensim.models.Phrases(text_list, min_count=10, threshold=50) #Generating bigrams\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_bigrams = make_bigrams(text_list) #add bigrams to paragraph word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words_bigrams\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "id2word.filter_extremes(no_below=4, no_above=0.5)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Similarity using gensim tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus\n",
    "from gensim.models import TfidfModel\n",
    "model = TfidfModel(corpus)\n",
    "vector = model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.similarities import MatrixSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = MatrixSimilarity(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0000001, 1.0000001, 0.       , 0.       , 1.0000001, 0.       ,\n",
       "       0.       , 1.0000001, 0.       ], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Example\n",
    "index[model[corpus[4]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Load test doc\n",
    "doc_df2 = pd.DataFrame()\n",
    "html_file = open(r\"C:\\Users\\arnab\\Desktop\\Berkeley\\MFE\\Sessions\\mfe_nlp\\royal_caribbean_10k.html\", 'r', encoding='utf-8',errors='ignore')\n",
    "html_body = html_file.read() \n",
    "clean_text2 = clean_html(html_body)\n",
    "doc_df2 = doc_df2.append(pd.Series([\"doc_\"+str(i),clean_text2]),ignore_index=True)\n",
    "doc_df2.columns = [\"doc_name\",\"text\"]\n",
    "\n",
    "text_list2 = []\n",
    "for i in notebook.tqdm(range(doc_df2.shape[0])):\n",
    "    text_blob2 = clean_text(doc_df2.text[i])\n",
    "    text_list2.extend([text_blob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "textArr2 = np.array(text_list2).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \" \".join(textArr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = [id2word.doc2bow(text) for text in text_list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.97311014 0.96655214 0.         0.97335756\n",
      "  0.9701577  0.         0.97659695]]\n"
     ]
    }
   ],
   "source": [
    "print(index[model[corpus2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Similarity using sklearn tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=.65, min_df=4, stop_words='english')\n",
    "transformed_documents = vectorizer.fit_transform(doc_df.text.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "# use this line of code to verify that the numpy array represents the same number of documents that we have in the file list\n",
    "len(transformed_documents_as_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_tf_idf_query_similarity(vectorizer, docs_tfidf, query):\n",
    "    \"\"\"\n",
    "    vectorizer: TfIdfVectorizer model\n",
    "    docs_tfidf: tfidf vectors for all docs\n",
    "    query: query doc\n",
    "\n",
    "    return: cosine similarity between query and all docs\n",
    "    \"\"\"\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    cosineSimilarities = cosine_similarity(query_tfidf, docs_tfidf).flatten()\n",
    "    return cosineSimilarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12082512, 0.12082512, 0.11491558, 0.11661261, 0.12082512,\n",
       "       0.11451854, 0.11330393, 0.12082512, 0.11310596])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tf_idf_query_similarity(vectorizer, transformed_documents, text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now deploy LDA model on the corpus, to arrive at a list of an optimum number of \"topics\" in the paragraphs. We use a multicore version of LDA to arrive at convergence faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=vector,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=500,\n",
    "                                           passes=20,\n",
    "                                           alpha=0.01,\n",
    "                                           eta='auto',\n",
    "                                           minimum_probability = 0.3,\n",
    "                                           minimum_phi_value=0.3,         \n",
    "                                           decay=0.5,\n",
    "                                           per_word_topics=True,workers=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"standardon\" + 0.003*\"specifyth\" + 0.003*\"taggedfact\" + 0.003*\"svg\" + 0.003*\"subtop\" + 0.003*\"stretch\" + 0.003*\"statementsbi\" + 0.003*\"startedth\"'),\n",
       " (1,\n",
       "  '0.014*\"search\" + 0.013*\"contentsitem\" + 0.012*\"swap\" + 0.011*\"fund\" + 0.011*\"pipelin_pipelin\" + 0.011*\"period_period\" + 0.011*\"instal_instal\" + 0.010*\"fraction_fraction\"'),\n",
       " (2,\n",
       "  '0.003*\"contentsitem\" + 0.003*\"search\" + 0.003*\"partner_partner\" + 0.003*\"period_period\" + 0.003*\"pipelin_pipelin\" + 0.003*\"origin_origin\" + 0.003*\"engin_engin\" + 0.003*\"background_imag\"'),\n",
       " (3,\n",
       "  '0.003*\"fund\" + 0.003*\"contentsitem\" + 0.003*\"pipelin_pipelin\" + 0.003*\"loan_loan\" + 0.003*\"instal_instal\" + 0.003*\"fraction_fraction\" + 0.003*\"translat_translat\" + 0.003*\"engin_engin\"'),\n",
       " (4,\n",
       "  '0.003*\"search\" + 0.003*\"background_imag\" + 0.003*\"popup\" + 0.003*\"webkit_box\" + 0.003*\"taxonomi_true\" + 0.003*\"deg\" + 0.003*\"set\" + 0.003*\"rgba\"')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_words=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Coherence scores </b> are a very common metric for scoring the relevance of an LDA model. These scores are based on the idea of co-occurence of topic words in sliding windows on the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.7140088108352263\n"
     ]
    }
   ],
   "source": [
    "# Compute   Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words_bigrams, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "#     coherence_values = []\n",
    "#     model_list = []\n",
    "#     for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word,random_state=100)\n",
    "#         model_list.append(model)\n",
    "#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "#     return model_list, coherence_values\n",
    "# # Can take a long time to run.\n",
    "# model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_words_bigrams, start=2, limit=40, step=6)\n",
    "# # Show graph\n",
    "# limit=40; start=2; step=6;\n",
    "# x = range(start, limit, step)\n",
    "# plt.plot(x, coherence_values)\n",
    "# plt.xlabel(\"Num Topics\")\n",
    "# plt.ylabel(\"Coherence score\")\n",
    "# plt.legend((\"coherence_values\"), loc='best')\n",
    "# plt.show()\n",
    "\n",
    "# # Print the coherence scores\n",
    "# for m, cv in zip(x, coherence_values):\n",
    "#     print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "    \n",
    "# # Select the model and print the topics\n",
    "# optimal_model = model_list[3]\n",
    "# model_topics = optimal_model.show_topics(formatted=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph Level Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a function to map the the top 3 most dominant topics for every paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_topics_paragraphs(ldamodel=lda_model, corpus=corpus, texts=texts):\n",
    "#     \"\"\"\n",
    "#     This function takes the lda model and the corpus as an input, so as to map the top topics and their percentages\n",
    "#     Input: lda_model,corpus, raw_text\n",
    "#     Output : Dataframe with top 3 topics and their percentages\n",
    "#     \"\"\"\n",
    "#     # Init output\n",
    "#     sent_topics_df = pd.DataFrame()\n",
    "\n",
    "#     # Get main topic in each document\n",
    "#     for i, row_list in tqdm(enumerate(ldamodel[corpus])):\n",
    "#         row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "#         # print(row)\n",
    "#         row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "#         # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "#         for j, (topic_num, prop_topic) in enumerate(row):\n",
    "#             if j == 0:  # => dominant topic\n",
    "#                 wp = ldamodel.show_topic(topic_num)\n",
    "#                 topic_keywords = \", \".join([word for word, prop in wp])\n",
    "#                 ser1 = pd.Series([int(topic_num), round(prop_topic,4)])\n",
    "#                # print(ser1)\n",
    "#             if j == 1:\n",
    "#                 ser1[2] = int(topic_num)\n",
    "#                 ser1[3] = round(prop_topic,4)\n",
    "#                 #print(ser1)\n",
    "#             if j == 2:\n",
    "#                 ser1[4] = int(topic_num)\n",
    "#                 ser1[5] = round(prop_topic,4)\n",
    "#                 #print(ser1)\n",
    "#         sent_topics_df = sent_topics_df.append(ser1,ignore_index = True)\n",
    "#         print(sent_topics_df)\n",
    "#     sent_topics_df.columns = ['topic1', 'contrib1','topic2', 'contrib2','topic3', 'contrib3']\n",
    "#     # Add original text to the end of the output\n",
    "#     contents = pd.Series(texts)\n",
    "#     sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "#     return(sent_topics_df)\n",
    "\n",
    "\n",
    "# df_topic_sents_keywords = format_topics_paragraphs(ldamodel=lda_model, corpus=corpus, texts=texts)\n",
    "\n",
    "# # Format\n",
    "# df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "# df_dominant_topic.columns = ['Document_No','topic1', 'contrib1','topic2', 'contrib2','topic3', 'contrib3', 'Text']\n",
    "# df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF vectorization entire text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df['Preprocessed'] = texts\n",
    "doc_df['Preprocessed'] = doc_df.apply(lambda x: \" \".join(x['Preprocessed']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>Preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_0</td>\n",
       "      <td>.picker_wrapper.no_alpha .picker_alpha{display...</td>\n",
       "      <td>display none posit_absolut index opac display ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>.picker_wrapper.no_alpha .picker_alpha{display...</td>\n",
       "      <td>display none posit_absolut index opac display ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_2</td>\n",
       "      <td>,term,score0,subsea,236.454126129562071,fmc,17...</td>\n",
       "      <td>term score subsea fmc surfac crude backlog sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_3</td>\n",
       "      <td>,term,score0,merchandise,138.547339529040271,k...</td>\n",
       "      <td>term score merchandis kohl store retail conten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_4</td>\n",
       "      <td>.picker_wrapper.no_alpha .picker_alpha{display...</td>\n",
       "      <td>display none posit_absolut index opac display ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_name                                               text  \\\n",
       "0    doc_0  .picker_wrapper.no_alpha .picker_alpha{display...   \n",
       "1    doc_1  .picker_wrapper.no_alpha .picker_alpha{display...   \n",
       "2    doc_2  ,term,score0,subsea,236.454126129562071,fmc,17...   \n",
       "3    doc_3  ,term,score0,merchandise,138.547339529040271,k...   \n",
       "4    doc_4  .picker_wrapper.no_alpha .picker_alpha{display...   \n",
       "\n",
       "                                        Preprocessed  \n",
       "0  display none posit_absolut index opac display ...  \n",
       "1  display none posit_absolut index opac display ...  \n",
       "2  term score subsea fmc surfac crude backlog sch...  \n",
       "3  term score merchandis kohl store retail conten...  \n",
       "4  display none posit_absolut index opac display ...  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.65, max_features=5000,\n",
       "                min_df=5, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000,max_df=.65, min_df=5, stop_words='english')\n",
    "tfidf_vect.fit(doc_df.Preprocessed.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = tfidf_vect.transform(doc_df.Preprocessed.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=500,\n",
       "                          mean_change_tol=0.001, n_components=3, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA = LatentDirichletAllocation(n_components=3, random_state=42,max_iter=500)\n",
    "LDA.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['liabil', 'impair', 'servic', 'fuel', 'percent', 'depreci', 'accru', 'stock', 'tax', 'equiti']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['adjust', 'defer', 'year', 'cash', 'amend', 'cost', 'incom', 'tabl', 'net', 'total']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['liabil', 'impair', 'fuel', 'percent', 'servic', 'tax', 'accru', 'depreci', 'stock', 'equiti']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Negative Matrix Factorization for Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=500,\n",
       "    n_components=6, random_state=42, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probability matrix that contains probabilities of all the words in the vocabulary for all the topics\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=6, random_state=42,max_iter=500)\n",
    "nmf.fit(doc_term_matrix )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['dilut', 'defer', 'year', 'cash', 'amend', 'cost', 'incom', 'tabl', 'net', 'total']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['plan', 'secur', 'total', 'asset', 'debt', 'liabil', 'decemb', 'revenu', 'tax', 'expens']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['expens_expens', 'earn', 'expens', 'plan', 'revenu', 'debt', 'loss', 'decemb', 'tax', 'liabil']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['revenu', 'liabil', 'secur', 'loss', 'plan', 'decemb', 'debt', 'expens_expens', 'asset', 'tax']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['payabl', 'earn', 'plan', 'secur', 'decemb', 'asset', 'liabil', 'expens_expens', 'debt', 'tax']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['differencesmay', 'diesel', 'diebold', 'dictat', 'diann', 'dhl', 'dform', 'dfe', 'differ', 'zone']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(nmf.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
